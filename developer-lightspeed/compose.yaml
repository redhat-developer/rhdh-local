services:
  # Dynamic plugins installer: Mounts Lightspeed configs for plugin installation
  install-dynamic-plugins:
    volumes:
      - ./developer-lightspeed/configs:/opt/app-root/src/developer-lightspeed/configs:Z
  
  # RHDH main service: Mounts Lightspeed configs for runtime access
  rhdh:
    volumes:
      - ./developer-lightspeed/configs:/opt/app-root/src/developer-lightspeed/configs:Z
  
  # RAG initialization service: Copies RAG embeddings and vector database to shared volumes
  # This runs once at startup to prepare the RAG data for Llama Stack
  rag-init:
    image: 'quay.io/redhat-ai-dev/rag-content:release-1.9-lcs'
    container_name: rag-init
    user: "root"
    volumes:
      - rag_embeddings:/data/embeddings_model
      - rag_vector_db:/data/vector_db
    entrypoint: [ "/bin/sh", "-c" ]
    command: |
      "set -e; echo 'Copying RAG data...' && \
      mkdir -p /data/vector_db /data/embeddings_model && \
      cp -r /rag/vector_db/* /data/vector_db/ && \
      cp -r /rag/embeddings_model/* /data/embeddings_model/ && \
      chown -R 1001:0 /data/vector_db /data/embeddings_model || true && \
      chmod -R a+rwX /data/vector_db /data/embeddings_model && \
      echo 'Copy complete.'"
    restart: "no"
  
  # Llama Stack Service
  # Provides the LLM inference and RAG capabilities for Developer Lightspeed
  # Default image: quay.io/redhat-ai-dev/llama-stack:0.1.4
  # To override: Set LLAMA_STACK_IMAGE in your .env file
  llama-stack:
    image: ${LLAMA_STACK_IMAGE:-quay.io/redhat-ai-dev/llama-stack:0.1.4} # dclint disable-line service-image-require-explicit-tag
    container_name: llama-stack
    network_mode: "service:rhdh"
    depends_on:
      rhdh:
        condition: service_started
    volumes:
      # Llama Stack configuration file (no safety guard mode)
      - ./developer-lightspeed/configs/extra-files/run-no-guard.yaml:/app-root/run.yaml:Z
      # RAG embeddings model for vector search
      - rag_embeddings:/rag-content/embeddings_model
      # RAG vector database for document search
      - rag_vector_db:/rag-content/vector_db
      # Vertex AI credentials (only used if ENABLE_VERTEX_AI=true)
      # Set VERTEX_AI_CREDENTIALS_PATH in your .env file to your Google Cloud credentials JSON file path
      - ${VERTEX_AI_CREDENTIALS_PATH:-./developer-lightspeed/configs/extra-files/templates/placeholder.json}:/app-root/credentials.json:Z
    environment:
      GOOGLE_APPLICATION_CREDENTIALS: /app-root/credentials.json
    env_file:
      - path: "./default.env"
        required: true
      - path: "./.env"
        required: false
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8321/v1/models || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s
  
  # Lightspeed Core Service
  # Main service that handles conversation management and coordinates with Llama Stack
  # Default image: quay.io/lightspeed-core/lightspeed-stack:0.4.0
  # To override: Set LIGHTSPEED_CORE_IMAGE in your .env file
  lightspeed-core-service:
    image: ${LIGHTSPEED_CORE_IMAGE:-quay.io/lightspeed-core/lightspeed-stack:0.4.0} # dclint disable-line service-image-require-explicit-tag
    container_name: lightspeed-core-service
    network_mode: "service:rhdh"
    depends_on:
      rhdh:
        condition: service_started
      llama-stack:
        condition: service_healthy
    volumes:
      # Lightspeed Core Service configuration file
      - ./developer-lightspeed/configs/extra-files/lightspeed-stack.yaml:/app-root/lightspeed-stack.yaml:Z
    env_file:
      - path: "./default.env"
        required: true
      - path: "./.env"
        required: false

volumes:
  user-data:
  rag_embeddings:
  rag_vector_db: