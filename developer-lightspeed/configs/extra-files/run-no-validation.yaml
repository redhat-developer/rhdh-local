# ==============================================================================
# Llama Stack Configuration - No Question Validation
# ==============================================================================
# This configuration file is used when question validation is disabled
# (selected "No validation" in step 2 of start-lightspeed.sh)
# It configures Llama Stack to provide LLM inference and RAG capabilities
# without filtering user questions for RHDH/Backstage relevance
#
# File location: developer-lightspeed/configs/extra-files/run-no-validation.yaml
# Mounted to: /app-root/run.yaml in the llama-stack container

version: '2'
image_name: llama-stack-no-question-validation

# ==============================================================================
# API Configuration
# ==============================================================================
# Defines which Llama Stack APIs are enabled
apis:
  - agents
  - datasetio
  - eval
  - inference
  - post_training
  - safety
  - scoring
  - telemetry
  - tool_runtime
  - vector_io
benchmarks: []
container_image: null
datasets: []
external_providers_dir: null

# ==============================================================================
# Inference Store Configuration
# ==============================================================================
# SQLite database for storing inference metadata and model information
inference_store:
  db_path: .llama/distributions/ollama/inference_store.db
  type: sqlite
logging: null
metadata_store:
  db_path: .llama/distributions/ollama/registry.db
  namespace: null
  type: sqlite

# ==============================================================================
# Provider Configuration
# ==============================================================================
# Defines inference, safety, and tool providers available to Llama Stack
# Providers are conditionally enabled based on environment variables
providers:
  agents:
  - config:
      persistence_store:
        db_path: .llama/distributions/ollama/agents_store.db
        namespace: null
        type: sqlite
      responses_store:
        db_path: .llama/distributions/ollama/responses_store.db
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference
  datasetio:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/huggingface_datasetio.db
        namespace: null
        type: sqlite
    provider_id: huggingface
    provider_type: remote::huggingface
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/localfs_datasetio.db
        namespace: null
        type: sqlite
    provider_id: localfs
    provider_type: inline::localfs
  eval:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/meta_reference_eval.db
        namespace: null
        type: sqlite
    provider_id: meta-reference
    provider_type: inline::meta-reference

  # ============================================================================
  # Inference Providers
  # ============================================================================
  # LLM inference providers - at least one must be enabled via environment variables
  # Providers are conditionally loaded based on ENABLE_* environment variables
  inference:
    - provider_id: ${env.ENABLE_VLLM:+vllm}
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL:=http://localhost/v1}
        api_token: ${env.VLLM_API_KEY:=token}
        max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
        tls_verify: ${env.VLLM_TLS_VERIFY:=true}
    - provider_id: ${env.ENABLE_OLLAMA:+ollama}
      provider_type: remote::vllm
      config:
        url: ${env.OLLAMA_URL:=http://localhost:11434/v1}
        api_token: ${env.OLLAMA_TOKEN:=token}
    - provider_id: ${env.ENABLE_OPENAI:+openai}
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY:=token}
    - provider_id: ${env.ENABLE_VERTEX_AI:+vertexai}
      provider_type: remote::vertexai
      config:
        project: ${env.VERTEX_AI_PROJECT:=}
        location: ${env.VERTEX_AI_LOCATION:=us-central1}
    - provider_id: sentence-transformers
      provider_type: inline::sentence-transformers
      config: {}
  post_training:
  - config:
      checkpoint_format: huggingface
      device: cpu
      distributed_backend: null
      dpo_output_dir: "."
    provider_id: huggingface
    provider_type: inline::huggingface
  safety: []
  scoring:
  - config: {}
    provider_id: basic
    provider_type: inline::basic
  - config: {}
    provider_id: llm-as-judge
    provider_type: inline::llm-as-judge
  - config:
      openai_api_key: '********'
    provider_id: braintrust
    provider_type: inline::braintrust
  telemetry:
  - config:
      service_name: 'lightspeed-stack-telemetry'
      sinks: sqlite
      sqlite_db_path: .llama/distributions/ollama/trace_store.db
    provider_id: meta-reference
    provider_type: inline::meta-reference
  tool_runtime:
  # Model Context Protocol (MCP) Provider
  # Enables Developer Lightspeed to interact with external tools and data sources
  - provider_id: model-context-protocol
    provider_type: remote::model-context-protocol
    config: {}
  # RAG Runtime Provider
  # Handles retrieval-augmented generation for RHDH documentation search
  - provider_id: rag-runtime 
    provider_type: inline::rag-runtime
    config: {}
  vector_io:
  - config:
      kvstore:
        db_path: .llama/distributions/ollama/faiss_store.db
        namespace: null
        type: sqlite
    provider_id: faiss
    provider_type: inline::faiss
  # RHDH Documentation Vector Database
  # Pre-loaded vector database containing RHDH product documentation for RAG
  - provider_id: rhdh-docs 
    provider_type: inline::faiss
    config:
      kvstore:
        type: sqlite
        namespace: null
        db_path: /app-root/vector_db/rhdh_product_docs/1.8/faiss_store.db

# ==============================================================================
# Server Configuration
# ==============================================================================
# Llama Stack API server settings
scoring_fns: []
server:
  auth: null
  host: null
  port: 8321
  quota: null
  tls_cafile: null
  tls_certfile: null
  tls_keyfile: null

# ==============================================================================
# Safety Shields
# ==============================================================================
# Safety shields are disabled in no-validation mode
# In validation mode, this would include question validation shields
shields: []

# ==============================================================================
# Tool Groups
# ==============================================================================
# Defines groups of tools available to the LLM
tool_groups:
- provider_id: rag-runtime
  toolgroup_id: builtin::rag
  description: "Only use for questions specifically about Red Hat Developer Hub (RHDH). Searches technical documentation for RHDH installation, discovery, configuration, release, upgrade, control access, integration, observability, and extending with plugins. Do not use for any other topic outside RHDH."

# ==============================================================================
# Vector Database Configuration
# ==============================================================================
# Vector databases for semantic search and RAG
vector_dbs:
- embedding_dimension: 768
  embedding_model: /app-root/embeddings_model
  provider_id: rhdh-docs
  vector_db_id: rhdh-product-docs-1_8

# ==============================================================================
# Model Configuration
# ==============================================================================
# Embedding models for vector search
models:
- model_id: sentence-transformers/all-mpnet-base-v2
  metadata:
      embedding_dimension: 768
  model_type: embedding
  provider_id: sentence-transformers
  provider_model_id: "/app-root/embeddings_model"
