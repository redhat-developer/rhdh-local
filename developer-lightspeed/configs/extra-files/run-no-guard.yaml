# ==============================================================================
# Llama Stack Configuration - Without Safety Guard
# ==============================================================================
# This configuration file is used when safety guard is diasbeld
# (selected "No safety guard" in step 2 of start-lightspeed.sh)
# It configures Llama Stack to provide LLM inference and RAG capabilities
# without safety guard to filter user questions for safety content
#
# File location: developer-lightspeed/configs/extra-files/run-no-guard.yaml
# Mounted to: /app-root/run.yaml in the llama-stack container

version: '2'
image_name: llama-stack-no-guard

# ==============================================================================
# API Configuration
# ==============================================================================
# Defines which Llama Stack APIs are enabled
apis:
  - agents
  - inference
  - safety
  - tool_runtime
  - vector_io
  - files
container_image:
external_providers_dir:

# ==============================================================================
# Provider Configuration
# ==============================================================================
# Defines inference, safety, and tool providers available to Llama Stack
# Providers are conditionally enabled based on environment variables
providers:
  agents:
    - config:
        persistence:
          agent_state:
            namespace: agents
            backend: kv_default
          responses:
            table_name: responses
            backend: sql_default
      provider_id: meta-reference
      provider_type: inline::meta-reference
  files:
    - provider_id: localfs
      provider_type: inline::localfs
      config:
        storage_dir: /tmp/llama-stack-files
        metadata_store:
          table_name: files_metadata
          backend: sql_files

  # ============================================================================
  # Inference Providers
  # ============================================================================
  # LLM inference providers - at least one must be enabled via environment variables
  # Providers are conditionally loaded based on ENABLE_* environment variables
  inference:
    - provider_id: ${env.ENABLE_VLLM:+vllm}
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL:=http://localhost/v1}
        api_token: ${env.VLLM_API_KEY:=token}
        max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
        tls_verify: ${env.VLLM_TLS_VERIFY:=true}
    - provider_id: ${env.ENABLE_OLLAMA:+ollama}
      provider_type: remote::ollama
      config:
        url: ${env.OLLAMA_URL:=http://localhost:11434}
    - provider_id: ${env.ENABLE_OPENAI:+openai}
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY:=token}
    - provider_id: ${env.ENABLE_VERTEX_AI:+vertexai}
      provider_type: remote::vertexai
      config:
        project: ${env.VERTEX_AI_PROJECT:=}
        location: ${env.VERTEX_AI_LOCATION:=us-central1}
    - provider_id: sentence-transformers
      provider_type: inline::sentence-transformers
      config: {}  
  tool_runtime:
    # Model Context Protocol (MCP) Provider
    # Enables Developer Lightspeed to interact with external tools and data sources
    - provider_id: model-context-protocol
      provider_type: remote::model-context-protocol
      config: {}
    # RAG Runtime Provider
    # Handles retrieval-augmented generation for RHDH documentation search
    - provider_id: rag-runtime
      provider_type: inline::rag-runtime
      config: {}
  vector_io:
    - provider_id: faiss
      provider_type: inline::faiss
      config:
        persistence:
          namespace: vector_io::faiss
          backend: faiss_kv

registered_resources:
  models:
    - model_id: sentence-transformers/all-mpnet-base-v2
      metadata:
        embedding_dimension: 768
      model_type: embedding
      provider_id: sentence-transformers
      provider_model_id: /rag-content/embeddings_model
  # Defines groups of tools available to the LLM
  tool_groups:
    - provider_id: rag-runtime
      toolgroup_id: builtin::rag
  # Vector databases for semantic search and RAG
  vector_dbs:
    - vector_db_id: rhdh-product-docs-1_8
      embedding_model: sentence-transformers/all-mpnet-base-v2
      embedding_dimension: 768
      provider_id: faiss

# ==============================================================================
# Server Configuration
# ==============================================================================
# Llama Stack API server settings
server:
  auth:
  host:
  port: 8321
  quota:
  tls_cafile:
  tls_certfile:
  tls_keyfile:

storage:
  backends:
    kv_default:
      type: kv_sqlite
      db_path: /tmp/kvstore.db
    sql_default:
      type: sql_sqlite
      db_path: /tmp/sql_store.db
    sql_files:
      type: sql_sqlite
      db_path: /rag-content/vector_db/rhdh_product_docs/1.9/files_metadata.db
    faiss_kv:
      type: kv_sqlite
      db_path: /rag-content/vector_db/rhdh_product_docs/1.9/faiss_store.db
  stores:
    metadata:
      namespace: registry
      backend: faiss_kv
    inference:
      table_name: inference_store
      backend: sql_default
      max_write_queue_size: 10000
      num_writers: 4
    conversations:
      table_name: openai_conversations
      backend: sql_default
