services:
  install-dynamic-plugins:
    volumes:
      - ./developer-lightspeed/configs:/opt/app-root/src/developer-lightspeed/configs:Z

  rhdh:
    volumes:
      - ./developer-lightspeed/configs:/opt/app-root/src/developer-lightspeed/configs:Z

  rag-init:
    image: 'quay.io/redhat-ai-dev/rag-content:release-1.7-lcs'
    container_name: rag-init
    user: "root"
    volumes:
      - rag_embeddings:/data/embeddings_model
      - rag_vector_db:/data/vector_db
    entrypoint: [ "/bin/sh", "-c" ]
    command: |
      "set -e; echo 'Copying RAG data...' && \
      mkdir -p /data/vector_db /data/embeddings_model && \
      cp -r /rag/vector_db/* /data/vector_db/ && \
      cp -r /rag/embeddings_model/* /data/embeddings_model/ && \
      chown -R 1001:0 /data/vector_db /data/embeddings_model || true && \
      chmod -R a+rwX /data/vector_db /data/embeddings_model && \
      echo 'Copy complete.'"
    restart: "no"

  ollama:
    image: docker.io/ollama/ollama:0.10.0
    container_name: ollama
    volumes:
      - ${OLLAMA_MODELS_PATH:-ollama_data}:/root/.ollama
    env_file:
      - path: "./default.env"
        required: true
      - path: "./.env"
        required: false
    entrypoint: [ "sh", "-c" ]
    command:
      - |
        set -e
        
        # Start the server in the background
        ollama serve &
        pid=$$!

        # Wait for the server to be ready using ollama list command
        echo "Waiting for Ollama server to start..."
        max_attempts=60
        attempt=0
        while [ $$attempt -lt $$max_attempts ]; do
          if ollama list >/dev/null 2>&1; then
            echo "Ollama server is ready."
            break
          fi
          attempt=$$((attempt + 1))
          sleep 1
        done

        if [ $$attempt -eq $$max_attempts ]; then
          echo "ERROR: Ollama server failed to start within $$max_attempts seconds"
          exit 1
        fi

        # Pull the model in the background
        echo "Pulling model: $${OLLAMA_MODEL:-llama3.2:1b}"
        ollama pull "$${OLLAMA_MODEL:-llama3.2:1b}" &
        
        # Wait for the server process to exit
        wait "$$pid"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "ollama", "list" ]
      interval: 10s
      timeout: 5s
      retries: 5

  llama-stack:
    image: ${LLAMA_STACK_IMAGE:-quay.io/redhat-ai-dev/llama-stack:6b98aa4ac2178e35d33ef0078bb948202e7dfabc} # dclint disable-line service-image-require-explicit-tag
    container_name: llama-stack
    network_mode: "service:rhdh"
    depends_on:
      rhdh:
        condition: service_started
    volumes:
      - ./developer-lightspeed/configs/extra-files/run-no-validation.yaml:/app-root/run.yaml:Z
      - rag_embeddings:/app-root/embeddings_model
      - rag_vector_db:/app-root/vector_db
    env_file:
      - path: "./default.env"
        required: true
      - path: "./.env"
        required: false
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8321/v1/models || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s

  lightspeed-core-service:
    image: ${LIGHTSPEED_CORE_IMAGE:-quay.io/lightspeed-core/lightspeed-stack:dev-20251021-ee9f08f} # dclint disable-line service-image-require-explicit-tag
    container_name: lightspeed-core-service
    depends_on:
      rhdh:
        condition: service_started
      ollama:
        condition: service_healthy
      llama-stack:
        condition: service_healthy
    volumes:
      - ./developer-lightspeed/configs/extra-files/lightspeed-core-service-config.yaml:/app-root/lightspeed-stack.yaml:Z
    env_file:
      - path: "./default.env"
        required: true
      - path: "./.env"
        required: false
    network_mode: "service:rhdh"

volumes:
  ollama_data:
  rag_embeddings:
  rag_vector_db: