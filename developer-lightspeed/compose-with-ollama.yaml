services:
  install-dynamic-plugins:
    volumes:
      - ./developer-lightspeed/configs:/opt/app-root/src/developer-lightspeed/configs:Z

  rhdh:
    volumes:
      - ./developer-lightspeed/configs:/opt/app-root/src/developer-lightspeed/configs:Z

  rag-init:
    image: 'quay.io/redhat-ai-dev/rag-content:release-1.8-lcs'
    container_name: rag-init
    user: "root"
    volumes:
      - rag_embeddings:/data/embeddings_model
      - rag_vector_db:/data/vector_db
    entrypoint: [ "/bin/sh", "-c" ]
    command: |
      "set -e; echo 'Copying RAG data...' && \
      mkdir -p /data/vector_db /data/embeddings_model && \
      cp -r /rag/vector_db/* /data/vector_db/ && \
      cp -r /rag/embeddings_model/* /data/embeddings_model/ && \
      chown -R 1001:0 /data/vector_db /data/embeddings_model || true && \
      chmod -R a+rwX /data/vector_db /data/embeddings_model && \
      echo 'Copy complete.'"
    restart: "no"

  # Ollama Service
  # Provides local LLM inference using Ollama
  # Default image: docker.io/ollama/ollama:0.10.0
  # Automatically pulls and serves the model specified by OLLAMA_MODEL environment variable
  ollama:
    image: docker.io/ollama/ollama:0.17.5
    container_name: ollama
    volumes:
      # Ollama models storage: Uses OLLAMA_MODELS_PATH from .env if set, otherwise uses named volume
      - ${OLLAMA_MODELS_PATH:-ollama_data}:/root/.ollama
    env_file:
      - path: "./default.env"
        required: true
      - path: "./.env"
        required: false
    # Command: Start Ollama server, wait 5s, pull the model, then mark as ready
    # Model is specified by OLLAMA_MODEL env var (default: llama3.2:1b)
    command: >
      "ollama serve & sleep 5 && ollama pull ${OLLAMA_MODEL:-llama3.2:1b} && touch /tmp/ollama-model-ready && wait"
    entrypoint: [ "sh", "-c" ]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q $$OLLAMA_MODEL && [ -f /tmp/ollama-model-ready ]"]
      interval: 10s
      timeout: 10s
      retries: 40
      start_period: 60s

  # Llama Stack Service
  # Provides the LLM inference and RAG capabilities for Developer Lightspeed
  # Default image: quay.io/redhat-ai-dev/llama-stack:6b98aa4ac2178e35d33ef0078bb948202e7dfabc
  # To override: Set LLAMA_STACK_IMAGE in your .env file
  llama-stack:
    image: ${LLAMA_STACK_IMAGE:-quay.io/redhat-ai-dev/llama-stack:6b98aa4ac2178e35d33ef0078bb948202e7dfabc} # dclint disable-line service-image-require-explicit-tag
    container_name: llama-stack
    network_mode: "service:rhdh"
    depends_on:
      rhdh:
        condition: service_started
      ollama:
        condition: service_healthy
    volumes:
      # Llama Stack configuration file (no question validation mode)
      - ./developer-lightspeed/configs/extra-files/run-no-validation.yaml:/app-root/run.yaml:Z
      # RAG embeddings model for vector search
      - rag_embeddings:/app-root/embeddings_model
      # RAG vector database for document search
      - rag_vector_db:/app-root/vector_db
    env_file:
      - path: "./default.env"
        required: true
      - path: "./.env"
        required: false
    environment:
      ENABLE_OLLAMA: 'true'
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8321/v1/models || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Lightspeed Core Service
  # Main service that handles conversation management and coordinates with Llama Stack
  # Default image: quay.io/lightspeed-core/lightspeed-stack:dev-20251021-ee9f08f
  # To override: Set LIGHTSPEED_CORE_IMAGE in your .env file
  lightspeed-core-service:
    image: ${LIGHTSPEED_CORE_IMAGE:-quay.io/lightspeed-core/lightspeed-stack:dev-20251021-ee9f08f} # dclint disable-line service-image-require-explicit-tag
    container_name: lightspeed-core-service
    depends_on:
      rhdh:
        condition: service_started
      ollama:
        condition: service_healthy
      llama-stack:
        condition: service_healthy
    volumes:
      # Lightspeed Core Service configuration file
      - ./developer-lightspeed/configs/extra-files/lightspeed-stack.yaml:/app-root/lightspeed-stack.yaml:Z
    env_file:
      - path: "./default.env"
        required: true
      - path: "./.env"
        required: false
    network_mode: "service:rhdh"

volumes:
  ollama_data:
  rag_embeddings:
  rag_vector_db: