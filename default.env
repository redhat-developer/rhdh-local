# Database connection
POSTGRES_DB=postgres
POSTGRES_HOST=db
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

BASE_URL=http://localhost:7007

# configure image to use: default to using the latest stable tag of the community builds, which are built for both amd64 and arm64 architectures
# to use bleeding edge :next images or commercially supported images, see README.md for details
# RHDH_IMAGE=quay.io/rhdh-community/rhdh:1.8

# GitHub auth (you need to uncomment github auth section in configs/app-config.local.yaml to enable this)
#AUTH_GITHUB_CLIENT_ID=
#AUTH_GITHUB_CLIENT_SECRET=

# Disable TLS certificate validation (development only)
NODE_TLS_REJECT_UNAUTHORIZED=0

#Â Use development segment key
SEGMENT_WRITE_KEY=gGVM6sYRK0D0ndVX22BOtS7NRcxPej8t
# uncomment the following line to disable telemetry
#SEGMENT_TEST_MODE=true

# Backstage log level
#LOG_LEVEL=debug

# Logs from global-agent to inspect the 'node-fetch' behavior with proxy settings
#ROARR_LOG=true

# Logs from fetch.
# Might be useful to inspect the 'fetch' behavior with proxy settings (handled by 'undici', not 'global-agent')
#NODE_DEBUG=fetch

# NO_PROXY will take effect only if HTTP(S)_PROXY env vars are set.
# See the compose-with-corporate-proxy.yaml file.
#NO_PROXY=localhost,127.0.0.1

# ==============================================================================
# Developer Lightspeed Configuration
# ==============================================================================
# For detailed documentation, see: developer-lightspeed/README.md
# Note: Ollama is pre-configured by default - no setup needed. External providers require configuration below.

# ------------------------------------------------------------------------------
# Ollama Provider Configuration (Default - Pre-configured)
# ------------------------------------------------------------------------------
# Pre-configured and ready to use - typically no changes needed
OLLAMA_URL=http://ollama:11434/v1
OLLAMA_TOKEN=dummy
OLLAMA_MODEL=llama3.2:1b               # Change to use different model
# OLLAMA_MODELS_PATH=/path/to/.ollama  # Optional: mount local models directory

# ------------------------------------------------------------------------------
# External Provider Enablement
# ------------------------------------------------------------------------------
# Set ONE to 'true' if using external provider instead of Ollama
ENABLE_VLLM=
ENABLE_OPENAI=
ENABLE_VERTEX_AI=

# ------------------------------------------------------------------------------
# vLLM Provider Configuration
# ------------------------------------------------------------------------------
# Required if ENABLE_VLLM=true
VLLM_URL=                    # Must end with /v1
VLLM_API_KEY=                # Leave empty if no auth required
# VLLM_MAX_TOKENS=4096       # Optional: max tokens (default: 4096)
# VLLM_TLS_VERIFY=true       # Optional: TLS verification (default: true)

# ------------------------------------------------------------------------------
# OpenAI Provider Configuration
# ------------------------------------------------------------------------------
# Required if ENABLE_OPENAI=true
OPENAI_API_KEY=              # Get from https://platform.openai.com/api-keys

# ------------------------------------------------------------------------------
# Vertex AI Provider Configuration (Experimental)
# ------------------------------------------------------------------------------
# Required if ENABLE_VERTEX_AI=true
VERTEX_AI_CREDENTIALS_PATH=  # Absolute path to GCP service account JSON
VERTEX_AI_PROJECT=           # Your GCP project ID
# VERTEX_AI_LOCATION=us-central1  # Optional: GCP region (default: us-central1)

# ------------------------------------------------------------------------------
# Question Validation Configuration
# ------------------------------------------------------------------------------
# Required if using "With validation" option (applies to all providers)
VALIDATION_PROVIDER=ollama   # Must match enabled provider: ollama|vllm|openai|vertexai
VALIDATION_MODEL=llama3.2:1b # Model must support large context windows
