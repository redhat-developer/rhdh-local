# database connection
POSTGRES_DB=postgres
POSTGRES_HOST=db
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

BASE_URL=http://localhost:7007

# configure image to use: default to using the latest stable tag of the community builds, which are build for both amd64 and arm64 architectures
# to use bleeding edge :next images or commercially supported images, see README.md for details
# RHDH_IMAGE=quay.io/rhdh-community/rhdh:1.8

# GitHub auth (you need to uncomment github auth section in configs/app-config.local.yaml to enable this)
#AUTH_GITHUB_CLIENT_ID=
#AUTH_GITHUB_CLIENT_SECRET=

NODE_TLS_REJECT_UNAUTHORIZED=0

#Â use development segment key
SEGMENT_WRITE_KEY=gGVM6sYRK0D0ndVX22BOtS7NRcxPej8t
# uncomment the following line to disable telemetry
#SEGMENT_TEST_MODE=true

# Backstage log level
#LOG_LEVEL=debug

# Logs from global-agent to inspect the 'node-fetch' behavior with proxy settings
#ROARR_LOG=true

# Logs from fetch.
# Might be useful to inspect the 'fetch' behavior with proxy settings (handled by 'undici', not 'global-agent')
#NODE_DEBUG=fetch

# NO_PROXY will take effect only if HTTP(S)_PROXY env vars are set.
# See the compose-with-corporate-proxy.yaml file.
#NO_PROXY=localhost,127.0.0.1

# Developer lightspeed server (Ollama Default)
# For more information, see:
# https://github.com/redhat-developer/rhdh-local/blob/main/developer-lightspeed/README.md

# Enable Inference Providers
## Set any providers you want enabled to 'true'
## E.g. ENABLE_VLLM=true
## Leave all disabled providers EMPTY
## E.g. ENABLE_OPENAI=
ENABLE_VLLM=true
ENABLE_OPENAI=

# vLLM Inference Settings
VLLM_URL=
VLLM_API_KEY=
# vLLM Optional Variables
VLLM_MAX_TOKENS=
VLLM_TLS_VERIFY=

# OpenAI Inference Settings
OPENAI_API_KEY=

# Ollama Inference Settings
OLLAMA_URL=http://ollama:11434/v1
OLLAMA_TOKEN=dummy
# Name of the Ollama model to use (optional). Defaults to llama3.2:1b
OLLAMA_MODEL=llama3.2:1b
# Path to your local Ollama models directory (optional). Defaults to ollama-data volume.
# OLLAMA_MODELS_PATH=/absolute/path/to/your/.ollama

# Question Validation Safety Shield Settings
## Ensure VALIDATION_PROVIDER is one of your enabled Inference Providers
## E.g. VALIDATION_PROVIDER=vllm if ENABLE_VLLM=true
VALIDATION_PROVIDER=ollama
VALIDATION_MODEL=llama3.2:1b

# Images
LIGHTSPEED_CORE_IMAGE=quay.io/lightspeed-core/lightspeed-stack:dev-20251021-ee9f08f
LLAMA_STACK_IMAGE=quay.io/redhat-ai-dev/llama-stack:0.1.1