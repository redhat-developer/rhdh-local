# database connection
POSTGRES_DB=postgres
POSTGRES_HOST=db
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

BASE_URL=http://localhost:7007

# configure image to use: default to using the latest stable tag of the community builds, which are build for both amd64 and arm64 architectures
# to use bleeding edge :next images or commercially supported images, see README.md for details
# RHDH_IMAGE=quay.io/rhdh-community/rhdh:1.6

# GitHub auth (you need to uncomment github auth section in configs/app-config.local.yaml to enable this)
#AUTH_GITHUB_CLIENT_ID=
#AUTH_GITHUB_CLIENT_SECRET=

NODE_TLS_REJECT_UNAUTHORIZED=0

#Â use development segment key
SEGMENT_WRITE_KEY=gGVM6sYRK0D0ndVX22BOtS7NRcxPej8t
# uncomment the following line to disable telemetry
#SEGMENT_TEST_MODE=true

# Backstage log level
#LOG_LEVEL=debug

# Logs from global-agent to inspect the 'node-fetch' behavior with proxy settings
#ROARR_LOG=true

# Logs from fetch.
# Might be useful to inspect the 'fetch' behavior with proxy settings (handled by 'undici', not 'global-agent')
#NODE_DEBUG=fetch

# NO_PROXY will take effect only if HTTP(S)_PROXY env vars are set.
# See the compose-with-corporate-proxy.yaml file.
#NO_PROXY=localhost,127.0.0.1

# Developer lightspeed server (Ollama Default)
# For more information, see:
# https://github.com/redhat-developer/rhdh-local/blob/main/developer-lightspeed/README.md

LLM_SERVER_ID=ollama
LLM_SERVER_URL=http://localhost:11434/v1
LLM_SERVER_TOKEN=dummy

# Path to your local Ollama models directory (optional). Defaults to ollama-data volume.
# OLLAMA_MODELS_PATH=/absolute/path/to/your/.ollama
# Name of the Ollama model to use (optional). Defaults to tinyllama
# OLLAMA_MODEL=llama2:7b

# ROAD CORE SERIVCE IMAGE
# ROAD_CORE_IMAGE=quay.io/redhat-ai-dev/road-core-service:rcs-06302025-rhdh-1.6