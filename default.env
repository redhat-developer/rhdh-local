# Database connection
POSTGRES_DB=postgres
POSTGRES_HOST=db
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres

BASE_URL=http://localhost:7007

# configure image to use: default to using the latest stable tag of the community builds, which are built for both amd64 and arm64 architectures
# to use bleeding edge :next images or commercially supported images, see README.md for details
# RHDH_IMAGE=quay.io/rhdh-community/rhdh:1.8

# Default plugin catalog index image
# Requires RHDH 1.9+ to be handled.
CATALOG_INDEX_IMAGE=quay.io/rhdh/plugin-catalog-index:1.9

# GitHub auth (you need to uncomment github auth section in configs/app-config.local.yaml to enable this)
#AUTH_GITHUB_CLIENT_ID=
#AUTH_GITHUB_CLIENT_SECRET=

# Node environment for extensions plugin installation
NODE_ENV=development

# Disable TLS certificate validation (development only)
NODE_TLS_REJECT_UNAUTHORIZED=0

# Use development segment key
SEGMENT_WRITE_KEY=gGVM6sYRK0D0ndVX22BOtS7NRcxPej8t
# uncomment the following line to disable telemetry
#SEGMENT_TEST_MODE=true

# Backstage log level
#LOG_LEVEL=debug

# Logs from global-agent to inspect the 'node-fetch' behavior with proxy settings
#ROARR_LOG=true

# Logs from fetch.
# Might be useful to inspect the 'fetch' behavior with proxy settings (handled by 'undici', not 'global-agent')
#NODE_DEBUG=fetch

# NO_PROXY will take effect only if HTTP(S)_PROXY env vars are set.
# See the compose-with-corporate-proxy.yaml file.
#NO_PROXY=localhost,127.0.0.1

# ==============================================================================
# Developer Lightspeed Configuration
# ==============================================================================
# For detailed documentation, see: developer-lightspeed/README.md
# Note: Ollama is pre-configured by default - no setup needed. External providers require configuration below.

# ------------------------------------------------------------------------------
# Ollama Provider Configuration (Default - Pre-configured)
# ------------------------------------------------------------------------------
# Pre-configured and ready to use - typically no changes needed
OLLAMA_URL=http://ollama:11434/v1
OLLAMA_TOKEN=dummy
# Change to use different model
OLLAMA_MODEL=llama3.2:1b
# Optional: mount local models directory
# OLLAMA_MODELS_PATH=/path/to/.ollama

# ------------------------------------------------------------------------------
# External Provider Enablement
# ------------------------------------------------------------------------------
# Set ONE to 'true' if using external provider instead of Ollama
# ENABLE_VLLM=
# ENABLE_OPENAI=
# ENABLE_VERTEX_AI=

# ------------------------------------------------------------------------------
# vLLM Provider Configuration
# ------------------------------------------------------------------------------
# Required if ENABLE_VLLM=true
# Must end with /v1
VLLM_URL=
# Leave empty if no auth required
VLLM_API_KEY=
# Optional: max tokens (default: 4096)
# VLLM_MAX_TOKENS=4096
# Optional: TLS verification (default: true)
# VLLM_TLS_VERIFY=true

# ------------------------------------------------------------------------------
# OpenAI Provider Configuration
# ------------------------------------------------------------------------------
# Required if ENABLE_OPENAI=true
# Get from https://platform.openai.com/api-keys
OPENAI_API_KEY=

# ------------------------------------------------------------------------------
# Vertex AI Provider Configuration (Experimental)
# ------------------------------------------------------------------------------
# Required if ENABLE_VERTEX_AI=true
# Absolute path to GCP service account JSON
VERTEX_AI_CREDENTIALS_PATH=
# Your GCP project ID
VERTEX_AI_PROJECT=
# Optional: GCP region (default: us-central1)
# VERTEX_AI_LOCATION=us-central1

# ------------------------------------------------------------------------------
# Question Validation Configuration
# ------------------------------------------------------------------------------
# Required if using "With validation" option (applies to all providers)
# Must match enabled provider: ollama|vllm|openai|vertexai
VALIDATION_PROVIDER=ollama
# Model must support large context windows
VALIDATION_MODEL=llama3.2:1b
